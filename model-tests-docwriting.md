## 'deepseek-coder-v2:16b-lite-instruct-q5_1' and q4_k_m
 both fail miserably on the large context task (~16k)

## 'phi3:14b-medium-128k-instruct-q6_K'
  does *pretty* well, seems like the larger quants are better at keeping track of wtf is going on
  actually on second thought that might be placebo, done a few runs now and it seems inconclusive
 
